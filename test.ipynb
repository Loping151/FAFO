{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stl import mesh\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d as mplot3d\n",
    "\n",
    "# 1. 读取STL文件\n",
    "your_mesh = mesh.Mesh.from_file('data/thingiverse/thing-3591512-file-6420490.stl')\n",
    "\n",
    "# 2. 获取STL模型的尺寸信息\n",
    "min_x, max_x = np.min(your_mesh.x), np.max(your_mesh.x)\n",
    "min_y, max_y = np.min(your_mesh.y), np.max(your_mesh.y)\n",
    "min_z, max_z = np.min(your_mesh.z), np.max(your_mesh.z)\n",
    "\n",
    "# 计算宽度、高度和深度\n",
    "width = max_x - min_x\n",
    "height = max_y - min_y\n",
    "depth = max_z - min_z\n",
    "\n",
    "# 设置输出图像的宽度（可以根据需要更改）\n",
    "output_width = 800  # 设置输出图像的宽度，高度将根据宽高比自动计算\n",
    "output_height = int(output_width * (height / width))  # 计算高度以保持宽高比\n",
    "\n",
    "# 3. 使用matplotlib渲染图像，并设置图形尺寸和缓冲区分辨率\n",
    "fig = plt.figure(figsize=(output_width / 100, output_height / 100))\n",
    "fig.set_dpi(100)  # 设置缓冲区分辨率\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# 设置摄像机的位置\n",
    "ax.view_init(elev=20, azim=30)\n",
    "\n",
    "# 为模型的每个面设置一个颜色\n",
    "ax.add_collection3d(mplot3d.art3d.Poly3DCollection(your_mesh.vectors, facecolors='gray'))\n",
    "\n",
    "# 设置轴的限制\n",
    "margin = 10\n",
    "ax.set_xlim([min_x - margin, max_x + margin])\n",
    "ax.set_ylim([min_y - margin, max_y + margin])\n",
    "ax.set_zlim([min_z - margin, max_z + margin])\n",
    "\n",
    "# 隐藏坐标轴\n",
    "ax.axis('off')\n",
    "\n",
    "# 设置背景为白色\n",
    "ax.set_facecolor((1, 1, 1, 0))\n",
    "fig.patch.set_facecolor((1, 1, 1, 0))\n",
    "\n",
    "# 4. 将matplotlib图像转换为PIL Image\n",
    "fig.canvas.draw()\n",
    "img_arr = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "img = Image.fromarray(img_arr)\n",
    "\n",
    "# 如果你想保存图像\n",
    "img.save('output_image.png')\n",
    "\n",
    "# 关闭图形，以便不在屏幕上显示它\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/thingiverse/thing-3591512-file-6420490.stl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb 单元格 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#X12sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     img\u001b[39m.\u001b[39msave(path[:\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.png\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#X12sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m img, path[:\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.png\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#X12sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m render_obj(\u001b[39m'\u001b[39;49m\u001b[39mdata/thingiverse/thing-3591512-file-6420490.stl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb 单元格 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender_obj\u001b[39m(path):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m obj_file:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         lines \u001b[39m=\u001b[39m obj_file\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     vertices \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.conda/envs/data/lib/python3.9/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/thingiverse/thing-3591512-file-6420490.stl'"
     ]
    }
   ],
   "source": [
    "def render_obj(path):\n",
    "    with open(path, 'r') as obj_file:\n",
    "        lines = obj_file.readlines()\n",
    "\n",
    "    vertices = []\n",
    "    faces = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('v '):\n",
    "            parts = line.strip().split()\n",
    "            x, y, z = map(float, parts[1:])\n",
    "            vertices.append([x, y, z])\n",
    "        elif line.startswith('f '):\n",
    "            parts = line.strip().split()\n",
    "            face = [int(vertex.split('/')[0]) for vertex in parts[1:]]\n",
    "            faces.append(face)\n",
    "\n",
    "    vertices = np.array(vertices)\n",
    "    faces = np.array(faces)\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    for face in faces:\n",
    "        vertices_3d = vertices[face - 1]\n",
    "        ax.add_collection3d(mplot3d.art3d.Poly3DCollection([vertices_3d], facecolors='gray'))\n",
    "\n",
    "    min_x, max_x = np.min(vertices[:, 0]), np.max(vertices[:, 0])\n",
    "    min_y, max_y = np.min(vertices[:, 1]), np.max(vertices[:, 1])\n",
    "    min_z, max_z = np.min(vertices[:, 2]), np.max(vertices[:, 2])\n",
    "\n",
    "    margin = 10\n",
    "    ax.set_xlim([min_x - margin, max_x + margin])\n",
    "    ax.set_ylim([min_y - margin, max_y + margin])\n",
    "    ax.set_zlim([min_z - margin, max_z + margin])\n",
    "\n",
    "    ax.axis('off')\n",
    "    ax.set_facecolor((1, 1, 1, 0))\n",
    "    fig.patch.set_facecolor((1, 1, 1, 0))\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    img_arr = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "    img = Image.fromarray(img_arr)\n",
    "\n",
    "    img.save(path[:-4] + '.png')\n",
    "\n",
    "    return img, path[:-4] + '.png'\n",
    "\n",
    "render_obj('data/thingiverse/thing-3591512-file-6420490.stl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22db1e894b645f7bf9df7f7763be329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86291f35fe5f48029673ffc075b665ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b75429069264602991e9b2c692c2cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d3c1e6638842c3ad3f570e2dd1f39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4de67592d6b4416976dbe4dda34bf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df28278c8d94441b6d95942dd403940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "image = Image.open('r_32.png')\n",
    "\n",
    "def test(caption, image=image):\n",
    "    device = \"cuda\"\n",
    "    \n",
    "    model, transform = clip.load(\"ViT-B/32\", device=device)\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    text = clip.tokenize([\"a \"+caption]).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text)\n",
    "\n",
    "    similarity_score = (image_features @ text_features.T).mean()\n",
    "    return similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb08f2a5540446b95bebf60abcc3b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.HalfTensor) and weight type (torch.HalfTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb 单元格 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     generated_text \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m generated_text\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m caption_image(image)\n",
      "\u001b[1;32m/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb 单元格 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcaption_image\u001b[39m(image):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     inputs \u001b[39m=\u001b[39m processor(image, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(device, torch\u001b[39m.\u001b[39mfloat16)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     generated_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     generated_text \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m generated_text\n",
      "File \u001b[0;32m~/.conda/envs/data/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/data/lib/python3.9/site-packages/transformers/models/blip_2/modeling_blip_2.py:1850\u001b[0m, in \u001b[0;36mBlip2ForConditionalGeneration.generate\u001b[0;34m(self, pixel_values, input_ids, attention_mask, **generate_kwargs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_accelerate()\n\u001b[1;32m   1849\u001b[0m batch_size \u001b[39m=\u001b[39m pixel_values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m-> 1850\u001b[0m image_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvision_model(pixel_values, return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[1;32m   1851\u001b[0m image_attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(image_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mimage_embeds\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   1853\u001b[0m query_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery_tokens\u001b[39m.\u001b[39mexpand(image_embeds\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/data/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/data/lib/python3.9/site-packages/transformers/models/blip_2/modeling_blip_2.py:548\u001b[0m, in \u001b[0;36mBlip2VisionModel.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[39mif\u001b[39;00m pixel_values \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    546\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify pixel_values\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 548\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(pixel_values)\n\u001b[1;32m    550\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m    551\u001b[0m     inputs_embeds\u001b[39m=\u001b[39mhidden_states,\n\u001b[1;32m    552\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    553\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    554\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    555\u001b[0m )\n\u001b[1;32m    557\u001b[0m last_hidden_state \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/data/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/data/lib/python3.9/site-packages/transformers/models/blip_2/modeling_blip_2.py:112\u001b[0m, in \u001b[0;36mBlip2VisionEmbeddings.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m    110\u001b[0m batch_size \u001b[39m=\u001b[39m pixel_values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    111\u001b[0m target_dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_embedding\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdtype\n\u001b[0;32m--> 112\u001b[0m patch_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatch_embedding(pixel_values\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49mtarget_dtype))  \u001b[39m# shape = [*, width, grid, grid]\u001b[39;00m\n\u001b[1;32m    113\u001b[0m patch_embeds \u001b[39m=\u001b[39m patch_embeds\u001b[39m.\u001b[39mflatten(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    115\u001b[0m class_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_embedding\u001b[39m.\u001b[39mexpand(batch_size, \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(target_dtype)\n",
      "File \u001b[0;32m~/.conda/envs/data/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/data/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.conda/envs/data/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.HalfTensor) and weight type (torch.HalfTensor) should be the same"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16).to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_blip(image):\n",
    "\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    \n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CLIP' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb 单元格 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test(\u001b[39m'\u001b[39m\u001b[39mchair\u001b[39m\u001b[39m'\u001b[39m), test(\u001b[39m'\u001b[39m\u001b[39mdog\u001b[39m\u001b[39m'\u001b[39m), test(\u001b[39m'\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m'\u001b[39m), test(\u001b[39m'\u001b[39m\u001b[39mseat\u001b[39m\u001b[39m'\u001b[39m), test(\u001b[39m'\u001b[39m\u001b[39mcomputer\u001b[39m\u001b[39m'\u001b[39m), test(\u001b[39m'\u001b[39m\u001b[39mperson\u001b[39m\u001b[39m'\u001b[39m), test(\u001b[39m'\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_blip(image)\n",
      "\u001b[1;32m/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb 单元格 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_blip\u001b[39m(image):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     inputs \u001b[39m=\u001b[39m processor(image, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(device, torch\u001b[39m.\u001b[39mfloat16)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     generated_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs, max_new_tokens\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     generated_text \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/loping151/Disks/TiPlus7100/data/3D_funiture/test.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m generated_text\n",
      "File \u001b[0;32m~/.conda/envs/data/lib/python3.9/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CLIP' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "test('chair'), test('dog'), test('table'), test('seat'), test('computer'), test('person'), test('object')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
